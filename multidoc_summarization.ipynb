{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FeelOneE/Multi-doc-summarization/blob/main/multidoc_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_enqEdyBH_MH"
      },
      "outputs": [],
      "source": [
        "#df = pd.DataFrame(result)\n",
        "df = pd.read_csv('review_data.csv')\n",
        "#df.to_csv('review_data.csv',sep=',', na_rep='NaN')\n",
        "df.describe\n",
        "df[['starScore','title']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzfWunF-H_MK"
      },
      "outputs": [],
      "source": [
        "# cleasing 함수 \n",
        "import regex as re\n",
        "import numpy as np\n",
        "def cleasing(text):\n",
        "    repl =''\n",
        "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)' # 자음, 모음 제거\n",
        "    text = re.sub(pattern= pattern, repl=repl, string=text)\n",
        "    #pattern = '[^\\w\\s]' # 특수기호 제거\n",
        "    pattern = '[^가-히\\s]' # 특수기호 제거\n",
        "    text = re.sub(pattern= pattern, repl=repl, string=text)\n",
        "    pattern = '<[^>]*>' # html 제거\n",
        "    text = re.sub(pattern = pattern, repl='',string=text)\n",
        "    return text\n",
        "df['length'] = df.content.str.len()\n",
        "df['label'] = np.where(df['starScore']<5, 0, 1)\n",
        "df['clean_content'] = df['content'].map(lambda x: cleasing(x))\n",
        "df2 = df[ df.length > 30 ] \n",
        "df2 = df2[['clean_content','label']]\n",
        "df2[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeUinS6MH_ML"
      },
      "outputs": [],
      "source": [
        "df2['check_content'] = ''\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "BvUdYyw7H_ML"
      },
      "outputs": [],
      "source": [
        "from py_hanspell.hanspell import spell_checker\n",
        "check_result = []\n",
        "\n",
        "for index, item in df2.iterrows():\n",
        "    try:\n",
        "        df2['check_content'][index] = spell_checker.check(item['clean_content']).checked\n",
        "    except Exception as e: \n",
        "        print(\"error occur! : \",e)\n",
        "        print(item['clean_content'])\n",
        "        continue\n",
        "\n",
        "df2[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xosoh5TwH_ML"
      },
      "outputs": [],
      "source": [
        "dataset_all = [ ['',0] for _ in range(len(df2)+1) ]\n",
        "#print(dataset_all[0][0])\n",
        "index = 0\n",
        "for indexs, row in df2.iterrows():\n",
        "    #print(index)\n",
        "    dataset_all[index][0] = row['clean_content']\n",
        "    dataset_all[index][1] = row['label']\n",
        "    index = index+1\n",
        "\n",
        "dataset_all[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTxtlUBrH_MM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('train_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y84ydIGjH_MM"
      },
      "outputs": [],
      "source": [
        "dataset_all = df.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KSqd5mvH_MN",
        "outputId": "cf68e508-e22a-4d84-ac93-fcdabf676ff5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2931, 1954)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_length = len(dataset_all)\n",
        "train_length = int(data_length*0.6)\n",
        "\n",
        "dataset_train = dataset_all[0:train_length]\n",
        "dataset_test = dataset_all[train_length:data_length]\n",
        "len(dataset_train),len(dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARoMy7t3H_MO",
        "outputId": "4322ec55-201e-4839-bcdb-1d71d8a6230c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "  Using cached mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl (33.1 MB)\n",
            "Collecting numpy<1.17.0,>=1.8.2\n",
            "  Using cached numpy-1.16.6-cp39-cp39-win_amd64.whl\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Using cached graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\python39\\lib\\site-packages (from mxnet) (2.18.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\python39\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\python39\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2020.11.8)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in c:\\python39\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
            "Installing collected packages: numpy, graphviz, mxnet\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.7.0.post2 numpy-1.16.6\n",
            "  WARNING: The script f2py.exe is installed in 'C:\\Users\\pil\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.3.3 requires numpy>=1.17, but you have numpy 1.16.6 which is incompatible.\n",
            "tensorboard 2.4.1 requires requests<3,>=2.21.0, but you have requests 2.18.4 which is incompatible.\n",
            "kobart 0.4 requires torch==1.7.1, but you have torch 1.8.1 which is incompatible.\n",
            "WARNING: You are using pip version 21.0.1; however, version 21.1.3 is available.\n",
            "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n",
            "Collecting gluonnlp\n",
            "  Using cached gluonnlp-0.10.0-cp39-cp39-win_amd64.whl\n",
            "Requirement already satisfied: pandas in c:\\python39\\lib\\site-packages (1.2.1)\n",
            "Requirement already satisfied: tqdm in c:\\python39\\lib\\site-packages (4.53.0)\n",
            "Requirement already satisfied: cython in c:\\python39\\lib\\site-packages (from gluonnlp) (0.29.23)\n",
            "WARNING: You are using pip version 21.0.1; however, version 21.1.3 is available.\n",
            "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n",
            "Requirement already satisfied: packaging in c:\\users\\pil\\appdata\\roaming\\python\\python39\\site-packages (from gluonnlp) (20.9)\n",
            "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\pil\\appdata\\roaming\\python\\python39\\site-packages (from gluonnlp) (1.16.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pil\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\python39\\lib\\site-packages (from pandas) (2021.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\pil\\appdata\\roaming\\python\\python39\\site-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Requirement already satisfied: sentencepiece in c:\\python39\\lib\\site-packages (0.1.96)\n",
            "WARNING: You are using pip version 21.0.1; however, version 21.1.3 is available.\n",
            "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n",
            "Requirement already satisfied: transformers in c:\\users\\pil\\appdata\\roaming\\python\\python39\\site-packages (4.3.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python39\\lib\\site-packages (from transformers) (2021.3.17)\n",
            "Requirement already satisfied: sacremoses in c:\\python39\\lib\\site-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\python39\\lib\\site-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\python39\\lib\\site-packages (from transformers) (4.53.0)\n",
            "Collecting numpy>=1.17\n",
            "  Using cached numpy-1.21.0-cp39-cp39-win_amd64.whl (14.0 MB)\n",
            "Requirement already satisfied: requests in c:\\python39\\lib\\site-packages (from transformers) (2.18.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\pil\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in c:\\users\\pil\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\pil\\appdata\\roaming\\python\\python39\\site-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in c:\\python39\\lib\\site-packages (from requests->transformers) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\python39\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\python39\\lib\\site-packages (from requests->transformers) (1.22)\n",
            "Requirement already satisfied: six in c:\\python39\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in c:\\python39\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\python39\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.16.6\n",
            "    Uninstalling numpy-1.16.6:\n",
            "      Successfully uninstalled numpy-1.16.6\n",
            "Successfully installed numpy-1.21.0\n",
            "  WARNING: The script f2py.exe is installed in 'C:\\Users\\pil\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.4.1 requires requests<3,>=2.21.0, but you have requests 2.18.4 which is incompatible.\n",
            "mxnet 1.7.0.post2 requires numpy<1.17.0,>=1.8.2, but you have numpy 1.21.0 which is incompatible.\n",
            "kobart 0.4 requires torch==1.7.1, but you have torch 1.8.1 which is incompatible.\n",
            "WARNING: You are using pip version 21.0.1; however, version 21.1.3 is available.\n",
            "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n",
            "Requirement already satisfied: torch in c:\\python39\\lib\\site-packages (1.8.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\python39\\lib\\site-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\pil\\appdata\\roaming\\python\\python39\\site-packages (from torch) (1.21.0)\n",
            "WARNING: You are using pip version 21.0.1; however, version 21.1.3 is available.\n",
            "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet --user\n",
        "!pip install gluonnlp pandas tqdm --user\n",
        "!pip install sentencepiece --user\n",
        "!pip install transformers --user\n",
        "!pip install torch --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-VKATY_H_MO",
        "outputId": "abc41af9-31da-40e3-f72d-1df98ea85c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' 'C:\\Users\\pil\\AppData\\Local\\Temp\\pip-req-build-jz7rgn_o'\n",
            "WARNING: You are using pip version 21.0.1; however, version 21.1.3 is available.\n",
            "You should consider upgrading via the 'c:\\python39\\python.exe -m pip install --upgrade pip' command.\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to c:\\users\\pil\\appdata\\local\\temp\\pip-req-build-jz7rgn_o\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py): started\n",
            "  Building wheel for kobert (setup.py): finished with status 'done'\n",
            "  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=12769 sha256=bc5d67a71248ca405b1b10f12247eb9cf2f749a83410124a3f35f4868bc884c3\n",
            "  Stored in directory: C:\\Users\\pil\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-2773bmg2\\wheels\\0b\\20\\d8\\031374f3d29b5150c59c814bed091fca7d6d4c8218148bf286\n",
            "Successfully built kobert\n",
            "Installing collected packages: kobert\n",
            "Successfully installed kobert-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b8xqK5zH_MP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjhK8FZeH_MP"
      },
      "outputs": [],
      "source": [
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p49GNw_aH_MP"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRsYz4j_H_MP",
        "outputId": "abf4d2a4-43f4-4055-9b5e-5f5922f24c51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 4055632473211169911,\n",
              " name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 13474087696481933928\n",
              " physical_device_desc: \"device: XLA_CPU device\",\n",
              " name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 4951408640\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 18075274555864177038\n",
              " physical_device_desc: \"device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\",\n",
              " name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 18420437764985841757\n",
              " physical_device_desc: \"device: XLA_GPU device\"]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6Qk7K2yH_MQ",
        "outputId": "bd050f86-6421-4a9b-94f3-2666692fab72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg93wzOrH_MQ",
        "outputId": "35795d1e-884f-44f3-ad6e-038852adf2db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ]
        }
      ],
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T0ctSr-H_MQ",
        "outputId": "df68b801-a696-4aa6-b2a3-eedb9269fb63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NluTe3_YH_MQ"
      },
      "outputs": [],
      "source": [
        "#!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
        "#!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjpUQJd8H_MR"
      },
      "outputs": [],
      "source": [
        "#dataset_train = nlp.data.TSVDataset(\"ratings_train.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n",
        "#dataset_test = nlp.data.TSVDataset(\"ratings_test.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhzR_h8QH_MR",
        "outputId": "8a699bd2-c282-4471-8d80-7ecdcbdd27a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['쑥 들어간 제품들을 평소에 좋아해서 쑥 화장품으로 대부분 쓰려고 해요 아침 저녁으로 매일 매일 쓰기에 부담이 없고 좋습니다자연약쑥 진정 토너라 자극없이 쓰기에도 좋고 녹차 루이보스 캐모마일 쑥 페퍼민트 로즈마리 유칼리투스잎이 들어간 그린 허브 성분이라 피부 진정에도 도움이 되는거 같아요',\n",
              "  1.0],\n",
              " ['화장품 상자에 한번 더 뽁뽁이  포장으로  꼼꼼하게 담아져 있었고  화장품 용기는 플라스틱 용기인데  용량이 커서  한참 쓸 수 있어 좋네요 뚜껑을 열었을때  향기 좋네 하는 느낌이 없으니 향을 중요시  하는 분은  좋아하지  않을 듯 하네요바르고 조금 시간이 지나면  향이  없어지는 것  같아요용기에 적혀 있는 글씨가 너무 적어  안보였는데 세트 구입했더니 앰플인지 뭔지 모르겠는데  작은 샘플이 개가 있네요  싼 가격에  이 구성이면  좋네요',\n",
              "  1.0],\n",
              " ['대용량에 헐 감동입니다 개는 지인도 팍팍 쓰라고 선물했어요이건 화장솜에 듬뿍 하셔서 많이 하셔도 흡수는 잘됩니다 저는 얼굴에 흡수가 안되는 타입인데 제품이 흡수 잘되고요 약간의 알콜냄새는 살짝 스치는 정도 입니다 코로나에 다들 든데 이 가격에 정말 감사합니다 마구 쓰시고 싶은 분은 구입 하시고 쫀질한 느낌과 깊은 느낌분들은 살짝 고민하시고 클렌징으로 쓰고 싶다 하시는 분들은 차 쓰시고 고가의 스킨을 바르시는것도 좋을듯 합니다',\n",
              "  1.0],\n",
              " ['아직 사용전인데 오자마자 간병이모님이 머냐고 물어 보셔서 약쑥스킨이라고  하니 한번 보자고 해서 뚜껑 열어서 향 맞아보고 손에 조금만 부어 보라고 해서 부었더니 발라보시더니 금방 흡수되고 촉촉하다고 하시며 얼마냐고 물어 보셔서 써보시라고 그냥 하나 드렸네요 방에 환자도 뭔데 그리 크냐고 하시면서 어떻게 사냐고 물어보시고 다들 관심 보이네요 각질제거가 된다니 일단 닦토로 써보고 조으면 계속 사서 써보려구요',\n",
              "  0.0],\n",
              " ['프레쥬 진정토너 여름에 시원하게 토너팩 하기 좋아요 향은 쑥 냄새보다는 시트러스계열 레몬향이 나는 편이구요대용량인데다 이라서 한통은 냉장고에 넣어놓고 자외선 높은 날에 진정용으로 팩하면 시원하고 진정도 되고 좋습니다가격이 워낙 저렴해서 크게 기대 안했는데 피지 많이 올라오는 여름에 쓰기 좋은 토너입니다',\n",
              "  1.0]]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1qnrar_H_MR",
        "outputId": "640d6900-26e6-4e66-e785-c4a86c7d64ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['향이 독하지않고 산뜻하니 좋습니다 이 가격에 피부에 도움을 바라는건 언감생심이죠 세안만으로 없어지지 않는 비비크림 잔여물이나 씻어주고 피부에 해롭지나 않았으면 더 바랄게 없네요',\n",
              "  1.0],\n",
              " ['잘받았네요 사용전이지만 좋을거라 믿어요양이 많아 오래쓰겠네요', 1.0],\n",
              " ['산뜻하면서도  에센스같은 수분감이 있어요용량도 대용량이라 부담없이 쓸수있어 좋아요', 1.0],\n",
              " ['가격만큼은 하네요 여릉용 바디 로션 겸용으로 사용해야겠네요', 0.0],\n",
              " ['사용하는게 있어 아직 사용전이지만 상품평이 좋아 기대되네요', 1.0]]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_test[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx0Y-4-jH_MR",
        "outputId": "e0bffdce-5c78-49fd-d3b5-90b139a5c76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ]
        }
      ],
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH-sje43H_MS"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DowXlVkmH_MS"
      },
      "outputs": [],
      "source": [
        "## Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-H9sSOPH_MS",
        "outputId": "122d0e9e-0111-4cab-8052-448f63165cce"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Argument 'text' has incorrect type (expected str, got float)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-21-bca58955ac96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-18-a9b1e492c2b1>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair)\u001b[0m\n\u001b[0;32m      5\u001b[0m             bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-18-a9b1e492c2b1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m             bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1011\u001b[0m             \u001b[0mtext_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         \u001b[0mtokens_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m         \u001b[0mtokens_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    847\u001b[0m         \"\"\"\n\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m_tokenizer\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    851\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize_wordpiece\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                 \u001b[0msplit_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    647\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32msrc/gluonnlp/data/fast_bert_tokenizer.pyx\u001b[0m in \u001b[0;36mgluonnlp.data.fast_bert_tokenizer.BasicTokenizer.tokenize\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: Argument 'text' has incorrect type (expected str, got float)"
          ]
        }
      ],
      "source": [
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7jKFKUGH_MS"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC_BV3OQH_MS"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=2,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqxSUFCuH_MS"
      },
      "outputs": [],
      "source": [
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhu7Bda1H_MT"
      },
      "outputs": [],
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SI-0L9aH_MT"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t3P8sjaH_MT"
      },
      "outputs": [],
      "source": [
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-zeI36cH_MT"
      },
      "outputs": [],
      "source": [
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikKl1NpnH_MT"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7bHSeqiH_MT"
      },
      "outputs": [],
      "source": [
        "data_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQGXDIN2H_MT"
      },
      "outputs": [],
      "source": [
        "data_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cl-ZZf4H_MT"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "metadata": {
      "interpreter": {
        "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
      }
    },
    "colab": {
      "name": "multidoc-summarization.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}